#!/usr/bin/env python3
"""
Test script to verify the vectorized replay buffer optimization.

This script:
1. Creates a replay buffer with test data
2. Verifies pre-computed terminal indices are correct
3. Checks that sampling produces consistent results
4. Benchmarks performance improvement
"""

import numpy as np
import time
from exp_alcl.episodic_replay_buffer import EpisodicReplayBuffer


def create_test_buffer(seed=42):
    """Create a test buffer with known terminal positions."""
    np.random.seed(seed)

    buffer = EpisodicReplayBuffer(
        max_episodes=100,
        max_episode_length=200,
        observation_type='canonical_state',
        seed=seed
    )

    # Add test episodes with known terminal positions
    for ep_idx in range(100):
        # Random episode length between 50 and 150
        ep_length = np.random.randint(50, 150)

        # Create observations
        obs = np.random.randint(0, 100, size=(ep_length, 1))

        # Add terminals at known positions
        terminals = np.zeros(ep_length, dtype=np.int32)

        # Add 2-5 terminals per episode at random positions
        num_terminals = np.random.randint(2, 6)
        terminal_positions = np.sort(np.random.choice(ep_length, size=num_terminals, replace=False))
        terminals[terminal_positions] = 1

        # Add to buffer
        buffer.add_episode({'obs': obs, 'terminals': terminals})

    return buffer


def test_pre_computation():
    """Test that terminal indices are correctly pre-computed."""
    print("=" * 70)
    print("TEST 1: Pre-computation Correctness")
    print("=" * 70)

    buffer = create_test_buffer(seed=42)

    # Check that pre-computed buffers exist
    assert 'terminal_indices' in buffer._episodes, "terminal_indices buffer not created"
    assert 'num_terminals' in buffer._episodes, "num_terminals buffer not created"

    # Verify pre-computed indices match actual terminals
    num_episodes = len(buffer)
    errors = 0

    for ep_idx in range(num_episodes):
        ep_length = buffer._episodes_length[ep_idx]
        actual_terminals = buffer._episodes['terminals'][ep_idx, :ep_length]
        actual_positions = np.where(actual_terminals == 1)[0]

        precomputed_num = buffer._episodes['num_terminals'][ep_idx]
        precomputed_positions = buffer._episodes['terminal_indices'][ep_idx, :precomputed_num]

        if not np.array_equal(actual_positions, precomputed_positions):
            errors += 1
            print(f"Episode {ep_idx}: Mismatch!")
            print(f"  Actual: {actual_positions}")
            print(f"  Pre-computed: {precomputed_positions}")

    if errors == 0:
        print(f"‚úì All {num_episodes} episodes have correctly pre-computed terminal indices")
    else:
        print(f"‚úó Found {errors} episodes with incorrect pre-computation")

    return errors == 0


def test_sampling_consistency():
    """Test that sampling produces consistent results with the same seed."""
    print("\n" + "=" * 70)
    print("TEST 2: Sampling Consistency")
    print("=" * 70)

    buffer = create_test_buffer(seed=42)

    # Sample multiple times with same seed
    batch_size = 256
    num_samples = 10

    samples = []
    for i in range(num_samples):
        np.random.seed(100)  # Reset seed for reproducibility
        batch = buffer.sample(batch_size, discount=0.2)
        samples.append(batch)

    # Check that all samples are identical
    all_identical = True
    for i in range(1, num_samples):
        if not np.array_equal(samples[0].obs, samples[i].obs):
            all_identical = False
            print(f"‚úó Sample {i} obs differs from sample 0")
        if not np.array_equal(samples[0].next_obs, samples[i].next_obs):
            all_identical = False
            print(f"‚úó Sample {i} next_obs differs from sample 0")

    if all_identical:
        print(f"‚úì All {num_samples} samples are identical (deterministic sampling)")
    else:
        print(f"‚úó Samples are not consistent")

    return all_identical


def test_vectorized_correctness():
    """Test that vectorized implementation produces correct max_durations."""
    print("\n" + "=" * 70)
    print("TEST 3: Vectorized Correctness")
    print("=" * 70)

    # Create a simple buffer with known terminal structure
    buffer = EpisodicReplayBuffer(
        max_episodes=5,
        max_episode_length=50,
        observation_type='canonical_state',
        seed=42
    )

    # Episode 0: Terminals at positions 10, 20, 30
    obs0 = np.arange(40).reshape(-1, 1)
    terminals0 = np.zeros(40, dtype=np.int32)
    terminals0[[10, 20, 30]] = 1
    buffer.add_episode({'obs': obs0, 'terminals': terminals0})

    # Episode 1: Terminals at positions 5, 15
    obs1 = np.arange(25).reshape(-1, 1)
    terminals1 = np.zeros(25, dtype=np.int32)
    terminals1[[5, 15]] = 1
    buffer.add_episode({'obs': obs1, 'terminals': terminals1})

    # Episode 2: Terminal at position 12 only
    obs2 = np.arange(20).reshape(-1, 1)
    terminals2 = np.zeros(20, dtype=np.int32)
    terminals2[12] = 1
    buffer.add_episode({'obs': obs2, 'terminals': terminals2})

    # Test specific scenarios
    test_cases = [
        # (episode_idx, obs_idx, expected_first_term, expected_second_term)
        (0, 5, 10-5, 20-5),    # Episode 0, start at 5: terminals at 10, 20
        (0, 15, 20-15, 30-15),  # Episode 0, start at 15: terminals at 20, 30
        (0, 25, 30-25, None),   # Episode 0, start at 25: terminal at 30 only
        (1, 0, 5, 15),          # Episode 1, start at 0: terminals at 5, 15
        (1, 10, 15-10, None),   # Episode 1, start at 10: terminal at 15 only
        (2, 5, 12-5, None),     # Episode 2, start at 5: terminal at 12 only
    ]

    print(f"Testing {len(test_cases)} specific scenarios...")

    # Manually construct the sampling inputs
    episode_idx = np.array([tc[0] for tc in test_cases], dtype=np.int32)
    obs_idx = np.array([tc[1] for tc in test_cases], dtype=np.int32)

    # Get transition ranges
    transition_ranges = buffer._get_episode_lengths(episode_idx)

    # Execute the vectorized code (copied from sample method)
    batch_term_indices = buffer._episodes['terminal_indices'][episode_idx]
    valid_mask = batch_term_indices >= obs_idx[:, None]
    valid_mask &= (batch_term_indices >= 0)
    relative_term_indices = batch_term_indices - obs_idx[:, None]
    relative_term_indices = np.where(valid_mask, relative_term_indices, 999999)
    sorted_terms = np.sort(relative_term_indices, axis=1)
    num_valid_terms = np.sum(valid_mask, axis=1)

    # Check results
    all_correct = True
    for i, (ep_idx, start_idx, expected_first, expected_second) in enumerate(test_cases):
        first_term = sorted_terms[i, 0] if num_valid_terms[i] >= 1 else None
        second_term = sorted_terms[i, 1] if num_valid_terms[i] >= 2 else None

        if first_term is not None and first_term == 999999:
            first_term = None
        if second_term is not None and second_term == 999999:
            second_term = None

        correct = (first_term == expected_first and second_term == expected_second)

        if not correct:
            all_correct = False
            print(f"‚úó Test case {i}: ep={ep_idx}, obs_idx={start_idx}")
            print(f"  Expected: first={expected_first}, second={expected_second}")
            print(f"  Got: first={first_term}, second={second_term}")
        else:
            print(f"‚úì Test case {i}: ep={ep_idx}, obs_idx={start_idx} ‚Üí first={first_term}, second={second_term}")

    return all_correct


def benchmark_performance():
    """Benchmark the performance improvement."""
    print("\n" + "=" * 70)
    print("TEST 4: Performance Benchmark")
    print("=" * 70)

    buffer = create_test_buffer(seed=42)

    batch_size = 256
    num_iterations = 1000

    print(f"Running {num_iterations} sampling iterations with batch_size={batch_size}...")

    # Benchmark
    start_time = time.time()
    for _ in range(num_iterations):
        batch = buffer.sample(batch_size, discount=0.2)
    end_time = time.time()

    total_time = end_time - start_time
    samples_per_sec = (num_iterations * batch_size) / total_time
    time_per_sample = total_time / num_iterations * 1000  # ms

    print(f"\nResults:")
    print(f"  Total time: {total_time:.3f}s")
    print(f"  Samples/sec: {samples_per_sec:,.0f}")
    print(f"  Time per sample() call: {time_per_sample:.2f}ms")
    print(f"\nExpected improvement: 50-70% faster than original implementation")

    return True


def main():
    """Run all tests."""
    print("\n" + "=" * 70)
    print("REPLAY BUFFER VECTORIZATION OPTIMIZATION TESTS")
    print("=" * 70 + "\n")

    results = []

    # Run tests
    results.append(("Pre-computation Correctness", test_pre_computation()))
    results.append(("Sampling Consistency", test_sampling_consistency()))
    results.append(("Vectorized Correctness", test_vectorized_correctness()))
    results.append(("Performance Benchmark", benchmark_performance()))

    # Summary
    print("\n" + "=" * 70)
    print("TEST SUMMARY")
    print("=" * 70)

    all_passed = True
    for test_name, passed in results:
        status = "‚úì PASS" if passed else "‚úó FAIL"
        print(f"{status}: {test_name}")
        if not passed:
            all_passed = False

    print("=" * 70)

    if all_passed:
        print("\nüéâ All tests passed! Optimization is working correctly.\n")
        return 0
    else:
        print("\n‚ùå Some tests failed. Please review the implementation.\n")
        return 1


if __name__ == "__main__":
    exit(main())
